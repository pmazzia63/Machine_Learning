{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-20T21:31:58.345503Z","iopub.execute_input":"2022-08-20T21:31:58.345913Z","iopub.status.idle":"2022-08-20T21:31:58.355024Z","shell.execute_reply.started":"2022-08-20T21:31:58.345877Z","shell.execute_reply":"2022-08-20T21:31:58.353702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#=======================================================================================\n# Importing the libaries:\n#=======================================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom scipy import stats\nimport math \nimport warnings\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\n#========================\nordinal_encoder = OrdinalEncoder()\nimputer = SimpleImputer(strategy=\"median\")\nenc = OneHotEncoder(sparse=False,handle_unknown='ignore')\n#===========================\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', None)\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000\n#=======================================================================================","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:31:58.357497Z","iopub.execute_input":"2022-08-20T21:31:58.357925Z","iopub.status.idle":"2022-08-20T21:31:58.370384Z","shell.execute_reply.started":"2022-08-20T21:31:58.357881Z","shell.execute_reply":"2022-08-20T21:31:58.369308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1 - IMPORTATION OF THE DATA ###","metadata":{}},{"cell_type":"code","source":"def read_data():\n    train_data = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n    print(\"Train data imported successfully!!\")\n    print(\"-\"*50)\n    test_data = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n    print(\"Test data imported successfully!!\")\n    return train_data , test_data","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:31:58.373007Z","iopub.execute_input":"2022-08-20T21:31:58.373446Z","iopub.status.idle":"2022-08-20T21:31:58.384234Z","shell.execute_reply.started":"2022-08-20T21:31:58.373402Z","shell.execute_reply":"2022-08-20T21:31:58.383095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data , test_data = read_data()","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:31:58.385724Z","iopub.execute_input":"2022-08-20T21:31:58.386594Z","iopub.status.idle":"2022-08-20T21:31:58.436948Z","shell.execute_reply.started":"2022-08-20T21:31:58.386558Z","shell.execute_reply":"2022-08-20T21:31:58.435699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we need to drop the 'Id' columns which is useless for the analysis.","metadata":{}},{"cell_type":"code","source":"# Save the 'Id' column\ntest_ID = test_data['Id']\n\n# Now drop the 'Id' column since it's unnecessary for  the prediction process.\ntrain_data.drop(\"Id\", axis = 1, inplace = True)\ntest_data.drop(\"Id\", axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:31:58.440422Z","iopub.execute_input":"2022-08-20T21:31:58.440903Z","iopub.status.idle":"2022-08-20T21:31:58.453766Z","shell.execute_reply.started":"2022-08-20T21:31:58.440854Z","shell.execute_reply":"2022-08-20T21:31:58.452524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look the correlation matrix and the most important attribut.","metadata":{}},{"cell_type":"code","source":"corrmat = train_data.corr()\nk = 11 #number of variables for heatmap\ncorrelated_cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncorrelated_cols","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:31:58.472042Z","iopub.execute_input":"2022-08-20T21:31:58.472569Z","iopub.status.idle":"2022-08-20T21:31:58.482742Z","shell.execute_reply.started":"2022-08-20T21:31:58.472527Z","shell.execute_reply":"2022-08-20T21:31:58.481568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize= (15 , 15))\nsns.heatmap(corrmat,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:31:58.484033Z","iopub.execute_input":"2022-08-20T21:31:58.484456Z","iopub.status.idle":"2022-08-20T21:31:59.507312Z","shell.execute_reply.started":"2022-08-20T21:31:58.484413Z","shell.execute_reply":"2022-08-20T21:31:59.505920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2 - MODIFY THE DATA AND FILL THE MISSING VALUES ###","metadata":{}},{"cell_type":"code","source":"def check_missed_values(all_data):\n    all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\n    all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n    missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n    return missing_data","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:31:59.509031Z","iopub.execute_input":"2022-08-20T21:31:59.509662Z","iopub.status.idle":"2022-08-20T21:31:59.517332Z","shell.execute_reply.started":"2022-08-20T21:31:59.509613Z","shell.execute_reply":"2022-08-20T21:31:59.515819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We regroup all the data in order to apply the same transformation.","metadata":{}},{"cell_type":"code","source":"all_data = pd.concat([train_data, test_data]).reset_index(drop=True)\nsale_price = train_data[\"SalePrice\"]\nall_data.drop(columns = [\"SalePrice\"] , inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:31:59.522820Z","iopub.execute_input":"2022-08-20T21:31:59.523240Z","iopub.status.idle":"2022-08-20T21:31:59.549589Z","shell.execute_reply.started":"2022-08-20T21:31:59.523205Z","shell.execute_reply":"2022-08-20T21:31:59.548388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We divide the data into the object attribute and the numerical one.","metadata":{}},{"cell_type":"code","source":"numerical_data = all_data.dtypes[all_data.dtypes != \"object\"].index\nobject_data = all_data.dtypes[all_data.dtypes == \"object\"].index","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:31:59.550997Z","iopub.execute_input":"2022-08-20T21:31:59.551937Z","iopub.status.idle":"2022-08-20T21:31:59.558690Z","shell.execute_reply.started":"2022-08-20T21:31:59.551893Z","shell.execute_reply":"2022-08-20T21:31:59.557326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try to fill the missing values.","metadata":{}},{"cell_type":"code","source":"missing_data = check_missed_values(all_data[object_data])\nmissing_data","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:31:59.559944Z","iopub.execute_input":"2022-08-20T21:31:59.560556Z","iopub.status.idle":"2022-08-20T21:31:59.586605Z","shell.execute_reply.started":"2022-08-20T21:31:59.560522Z","shell.execute_reply":"2022-08-20T21:31:59.585491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_object = train_data[object_data]\ndata_object[missing_data.index] = data_object[missing_data.index].fillna(\"None\")\ndata_object = data_object.astype('str')","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:31:59.588027Z","iopub.execute_input":"2022-08-20T21:31:59.588464Z","iopub.status.idle":"2022-08-20T21:31:59.605289Z","shell.execute_reply.started":"2022-08-20T21:31:59.588430Z","shell.execute_reply":"2022-08-20T21:31:59.604153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_object =ordinal_encoder.fit_transform(data_object)\ndata_object = pd.DataFrame(data_object, columns=object_data)\ndata_correlation = pd.concat([data_object, sale_price], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:31:59.606489Z","iopub.execute_input":"2022-08-20T21:31:59.607070Z","iopub.status.idle":"2022-08-20T21:31:59.645102Z","shell.execute_reply.started":"2022-08-20T21:31:59.607035Z","shell.execute_reply":"2022-08-20T21:31:59.644171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_object = data_correlation.corr()","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:31:59.646537Z","iopub.execute_input":"2022-08-20T21:31:59.646928Z","iopub.status.idle":"2022-08-20T21:31:59.661784Z","shell.execute_reply.started":"2022-08-20T21:31:59.646856Z","shell.execute_reply":"2022-08-20T21:31:59.660703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k = 10 #number of variables for heatmap\ncorrelated_object_cols2 = corr_object['SalePrice'].abs().sort_values()\nimpt_object = correlated_object_cols2[-11:-1].index","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:31:59.663586Z","iopub.execute_input":"2022-08-20T21:31:59.664014Z","iopub.status.idle":"2022-08-20T21:31:59.670634Z","shell.execute_reply.started":"2022-08-20T21:31:59.663970Z","shell.execute_reply":"2022-08-20T21:31:59.669335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Any object seem relavant to the analysis.","metadata":{}},{"cell_type":"markdown","source":"### LET'S BUILD OUR MODEL ###","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:31:59.672306Z","iopub.execute_input":"2022-08-20T21:31:59.672997Z","iopub.status.idle":"2022-08-20T21:31:59.680631Z","shell.execute_reply.started":"2022-08-20T21:31:59.672922Z","shell.execute_reply":"2022-08-20T21:31:59.679738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"impt_num_data = all_data[correlated_cols[1:]]\nimpt_object_data =  all_data[impt_object].fillna(\"None\").astype(\"str\")","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:31:59.682023Z","iopub.execute_input":"2022-08-20T21:31:59.682430Z","iopub.status.idle":"2022-08-20T21:31:59.697275Z","shell.execute_reply.started":"2022-08-20T21:31:59.682386Z","shell.execute_reply":"2022-08-20T21:31:59.696298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"data_train_final_num, data_test_final_num= impt_num_data.iloc[1460,:],impt_num_data.iloc[1460:,:]\ndata_train_final_obj, data_test_final_obj= impt_object_data.iloc[1460,:],impt_object_data.iloc[1460:,:]","metadata":{}},{"cell_type":"markdown","source":"Let's fill missing value with a median number and scale the value !","metadata":{}},{"cell_type":"code","source":"pipeline = Pipeline([\n        ('std_scaler', StandardScaler()),\n        ('imputer', SimpleImputer(strategy=\"median\")),\n    ])\n\nimpt_num_data = pipeline.fit_transform(impt_num_data) \nimpt_object_data = ordinal_encoder.fit_transform(impt_object_data) \nimpt_object_data = pipeline.fit_transform(impt_object_data)\ndata_num, data_object = tf.convert_to_tensor(impt_num_data), tf.convert_to_tensor(impt_object_data)\ndata = tf.concat([data_num, data_object], 1)\nXtrain, Xtest = data[:1460,:],data[1460:,]\nY = tf.convert_to_tensor(sale_price)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:31:59.700139Z","iopub.execute_input":"2022-08-20T21:31:59.701080Z","iopub.status.idle":"2022-08-20T21:31:59.735080Z","shell.execute_reply.started":"2022-08-20T21:31:59.701047Z","shell.execute_reply":"2022-08-20T21:31:59.734093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We split finally the training and test data.","metadata":{}},{"cell_type":"code","source":"# Set random seed\ntf.random.set_seed(42)\n\n# Build the model (3 layers, 100, 10, 1 units)\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Dense(256, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.BatchNormalization(),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.BatchNormalization(),\n  tf.keras.layers.Dense(64, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.BatchNormalization(),\n  tf.keras.layers.Dense(1, activation=\"linear\")\n])\n\n\n## Compile the model\nmodel.compile(loss=tf.keras.losses.MeanSquaredError(),\n                          optimizer=tf.keras.optimizers.Adam(0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name='Adam'),\n                          metrics=[tf.keras.metrics.MeanSquaredLogarithmicError()])\n\n## Fit the model for 200 epochs (same as insurance_model_2)\nmodel.fit(Xtrain, Y, epochs=50) \n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:31:59.736313Z","iopub.execute_input":"2022-08-20T21:31:59.736617Z","iopub.status.idle":"2022-08-20T21:32:08.862306Z","shell.execute_reply.started":"2022-08-20T21:31:59.736588Z","shell.execute_reply":"2022-08-20T21:32:08.861251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_predi = np.array(tf.reduce_mean(model.predict(Xtest), 1))\nnp.array(test_ID)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:32:08.865403Z","iopub.execute_input":"2022-08-20T21:32:08.865756Z","iopub.status.idle":"2022-08-20T21:32:09.109123Z","shell.execute_reply.started":"2022-08-20T21:32:08.865722Z","shell.execute_reply":"2022-08-20T21:32:09.108024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4 - EXPORT THE RESULT ###","metadata":{}},{"cell_type":"code","source":"# Read in sample_submission dataframe\n\n\noutput = pd.DataFrame(columns=[\"Id\",\"SalePrice\"])\noutput[\"Id\"] = np.array(test_ID)\noutput[\"SalePrice\"] = Y_predi\noutput[\"Id\"] = output[\"Id\"].astype(\"int\")","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:32:09.110458Z","iopub.execute_input":"2022-08-20T21:32:09.110782Z","iopub.status.idle":"2022-08-20T21:32:09.120478Z","shell.execute_reply.started":"2022-08-20T21:32:09.110745Z","shell.execute_reply":"2022-08-20T21:32:09.119428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output.to_csv('/kaggle/working/submission.csv', index=False)\nprint('Submission succesful!')","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:32:09.121674Z","iopub.execute_input":"2022-08-20T21:32:09.122013Z","iopub.status.idle":"2022-08-20T21:32:09.135884Z","shell.execute_reply.started":"2022-08-20T21:32:09.121977Z","shell.execute_reply":"2022-08-20T21:32:09.134998Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
